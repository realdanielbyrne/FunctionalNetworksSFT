# Functional Networks SFT

**FunctionalNetworksSFT** is a research-oriented framework for fine-tuning large language models using ICA-based functional network masking. This innovative approach leverages neuroscience insights about functional brain networks to selectively train or ablate specific neuron groups during supervised fine-tuning (SFT).

## Core Innovation

The framework implements **functional network masking** - a technique that applies binary masks to specific neurons during training based on Independent Component Analysis (ICA) of neuron activations. This allows researchers to:

- **Ablate key networks**: Mask important functional networks to study their role in model performance
- **Isolate networks**: Train only specific functional networks while masking all others
- **Targeted fine-tuning**: Update weights only in functionally relevant neurons, potentially reducing negative effects of full parameter fine-tuning

## Key Capabilities

- **ICA-Based Network Discovery**: Automatically identifies functional networks using FastICA analysis of MLP activations
- **Flexible Masking Modes**: Support for both ablation (`key`) and isolation (`complement`) masking strategies
- **Pre-computed or On-the-fly ICA**: Use existing ICA masks from JSON files or compute them dynamically during training
- **Hugging Face Integration**: Seamless compatibility with transformers, LoRA/QLoRA, and quantization techniques
- **Cross-Platform Optimization**: Native support for CUDA, Apple Silicon (MPS), and CPU-only environments

## Features

- üöÄ **Cross-Platform Support**: Works on CUDA-enabled systems and Apple Silicon Macs
- ‚ö° **Hardware Acceleration**: Automatic detection and optimization for CUDA and MPS backends
- üîß **Quantization Support**: BitsAndBytes 4-bit/8-bit quantization on CUDA systems
- üéØ **LoRA/QLoRA**: Parameter-efficient fine-tuning with automatic target module detection
- üìä **Experiment Tracking**: Built-in Weights & Biases integration
- üîÑ **Flexible Data Formats**: Automatic detection and conversion of various dataset formats
- üõ°Ô∏è **Robust Error Handling**: Graceful fallbacks for platform-specific features

## Quick Start

### Installation

1. **Clone the repository:**

   ```bash
   git clone <repository-url>
   cd FunctionalNetworksSFT
   ```

2. **Install dependencies:**

   ```bash
   poetry install
   ```

3. **Platform-specific setup:**

   For detailed platform-specific installation instructions, see [CROSS_PLATFORM_SETUP.md](CROSS_PLATFORM_SETUP.md).

### Basic Usage

```bash
poetry run fnsft \
    --model_name_or_path microsoft/DialoGPT-medium \
    --dataset_name_or_path your_dataset.json \
    --output_dir ./output \
    --num_train_epochs 3 \
    --per_device_train_batch_size 4 \
    --torch_dtype auto
```

### Example: Fine-tuning Llama-3.2-1B on Sarcasm Dataset

This example demonstrates fine-tuning the Llama-3.2-1B model on a sarcasm detection dataset using intelligent chat template handling:

```bash
poetry run fnsft \
    --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
    --dataset_name_or_path sarcasm.csv \
    --output_dir ./fine-tuned-model \
    --template_format auto \
    --num_train_epochs 2 \
    --per_device_train_batch_size 2 \
    --per_device_eval_batch_size 2 \
    --learning_rate 1e-5 \
    --torch_dtype float32 \
    --validation_split 0.05 \
    --max_seq_length 128 \
    --eval_steps 40 \
    --logging_steps 40 \
    --save_steps 150 \
    --max_grad_norm 2
```

**Key features demonstrated:**

- **Automatic Chat Template Detection**: `--template_format auto` automatically detects and uses Llama's chat template
- **CSV Dataset Support**: Direct loading of CSV files with `question,answer` columns
- **MacBook Compatibility**: `--torch_dtype float32` ensures compatibility with Apple Silicon
- **Intelligent Data Splitting**: `--validation_split 0.05` automatically creates train/validation splits
- **Optimized for Small Models**: Batch sizes and sequence length optimized for 1B parameter models

## Platform Support

| Platform | GPU Acceleration | Quantization | Status |
|----------|------------------|--------------|--------|
| CUDA (NVIDIA) | ‚úÖ CUDA | ‚úÖ BitsAndBytes | Fully Supported |
| Apple Silicon | ‚úÖ MPS | ‚ùå Not Available | Supported |
| CPU Only | ‚ùå CPU Only | ‚ùå Not Available | Basic Support |

## Documentation

- **[Cross-Platform Setup Guide](CROSS_PLATFORM_SETUP.md)** - Detailed installation instructions for all platforms
- **[Training Guide](docs/training.md)** - Comprehensive training documentation (coming soon)
- **[API Reference](docs/api.md)** - API documentation (coming soon)

## Research

### ICA-Based Functional Network Masking for LLM Fine-Tuning

Large Language Models have been shown to contain functional networks of neurons that frequently co-activate and are crucial to model performance Ôøº. In this approach, we fine-tune a Hugging Face transformer model while masking out selected neurons based on an Independent Component Analysis (ICA) of neuron activations Ôøº. By toggling which functional networks (groups of neurons) are active during training, we can explore the model‚Äôs reliance on those networks. Below, we detail the implementation steps, CLI integration, use of precomputed ICA masks or on-the-fly computation, and how to integrate this extension with Hugging Face‚Äôs SFTTrainer (supporting LoRA and quantization). We also provide an example fine-tuning on a logic/code task to demonstrate usage.

#### Functional Network Masking in LLMs

Researchers have found that neurons in LLMs form functional networks analogous to functional brain networks Ôøº Ôøº. These are sets of neurons that consistently co-activate under certain conditions Ôøº. Crucially, only a small fraction of neurons may constitute key networks essential for performance: masking these key networks (setting their outputs to zero) significantly degrades model performance, whereas retaining only these networks (masking all others) can still maintain much of the model‚Äôs functionality Ôøº Ôøº. Prior work even showed that manipulating important neurons‚Äô outputs via amplification or masking can steer model behavior Ôøº.

Our goal is to leverage these insights by introducing binary neuron masks during fine-tuning. This mask will zero-out either a chosen functional network (to ablate it) or all but that network (to isolate it). The masking is applied in the forward pass to the outputs of specific neurons, thereby affecting which neurons contribute to model computations and which gradients are updated. This allows us to fine-tune the model with or without certain functional subnetworks, potentially leading to insights on their role or even a lighter model focusing on key neurons Ôøº.

#### Implementing Binary Neuron Masks in the Forward Pass

To apply binary masks during the forward pass, we inject PyTorch forward hooks at the appropriate locations in the model‚Äôs architecture. In transformer decoders, each block contains an MLP (feed-forward network) that expands to a higher-dimensional ‚Äúintermediate‚Äù layer (often 4√ó the hidden size) and then projects back to the hidden size Ôøº. We treat each dimension in this intermediate layer as a ‚Äúneuron‚Äù and target them for masking (since these are the neurons identified by ICA analysis Ôøº).

**Where to mask**: We apply the mask right after the non-linear activation in the MLP, just before the second linear projection back to the hidden size. By zeroing out selected intermediate features at this point, we effectively remove those neurons‚Äô contribution to the model‚Äôs output. This is equivalent to ‚Äúmasking their outputs,‚Äù as described in prior research Ôøº. It ensures masked neurons produce no activation and receive no gradient, while unmasked neurons function normally.

**How to mask via hooks**: We register a forward pre-hook on each MLP‚Äôs second linear layer (often named proj or down_proj), so that we can intercept its input. The hook multiplies the input tensor elementwise by a binary mask (1s for active neurons, 0s for masked neurons). Using a forward pre-hook means the mask is applied before the linear layer computes its output. This approach cleanly separates masking logic from the model code (no need to manually edit model source) and works with LoRA or quantized layers as long as we target the correct module.

**Verifying hook placement**: We identify the correct linear layer by checking for a linear whose out_features == hidden_size and in_features > hidden_size (since the intermediate layer is larger). This reliably catches the second projection of the MLP in architectures like GPT, LLaMA, OPT, BERT, etc., but skips attention output layers (which have in_features == out_features == hidden_size). For example, in GPT-2 each block‚Äôs MLP has c_fc (1024‚Üí4096) and c_proj (4096‚Üí1024); our code finds c_proj and masks its input of size 4096. In LLaMA-7B, each layer‚Äôs MLP uses two projections (gate_proj and up_proj both 4096‚Üí11008) and a down_proj (11008‚Üí4096); the hook targets down_proj‚Äôs input of size 11008. By masking that input, we zero out selected intermediate neurons after the SiLU activation and gating, which is effectively removing those functional units from the network‚Äôs computations.

**Trainable parameters**: With this forward masking in place, any neuron set to 0 will also receive zero gradient (since its output does not affect the loss). Thus, during fine-tuning those masked neurons‚Äô weights won‚Äôt update (their gradient is essentially suppressed). The rest of the model trains normally. This fulfills the requirement that the mask affects which neurons are active and trainable: masked ones are inactive (and effectively not trained), while unmasked ones continue to learn.

## Requirements

- Python 3.12+
- Poetry for dependency management
- Platform-specific requirements (see setup guide)

## License

MIT License - see LICENSE file for details.

## Contributing

Contributions are welcome! Please read our contributing guidelines before submitting pull requests.

## Support

For issues and questions:

1. Check the [Cross-Platform Setup Guide](CROSS_PLATFORM_SETUP.md)
2. Search existing GitHub issues
3. Create a new issue with platform details and error logs
