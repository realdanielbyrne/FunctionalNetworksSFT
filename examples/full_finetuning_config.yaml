# Full Parameter Fine-Tuning Configuration Example
# This configuration performs traditional full parameter fine-tuning
# WARNING: Requires significantly more memory and compute resources!

# Model configuration
model_name_or_path: "microsoft/DialoGPT-medium"
use_auth_token: true
trust_remote_code: true
torch_dtype: "auto"

# Dataset configuration
dataset_name_or_path: "tatsu-lab/alpaca"
dataset_config_name: null
max_seq_length: 2048
instruction_template: "### Instruction:\n{instruction}\n\n### Response:\n{response}"
validation_split: 0.1
auto_detect_format: true
template_format: "auto"

# Quantization settings (optional for full fine-tuning)
# Note: Quantization can still be used to reduce memory usage
use_4bit: false  # Disable for full precision training
use_8bit: false
bnb_4bit_compute_dtype: "float16"
bnb_4bit_quant_type: "nf4"
bnb_4bit_use_double_quant: true

# PEFT/LoRA configuration
no_peft: true  # Disable PEFT - use full parameter fine-tuning
# LoRA parameters are ignored when use_peft is false
lora_r: 16
lora_alpha: 32
lora_dropout: 0.1
lora_target_modules: null
lora_bias: "none"

# Training configuration (optimized for full fine-tuning)
output_dir: "./models/full-finetuned"
num_train_epochs: 2  # Fewer epochs to prevent overfitting
per_device_train_batch_size: 2  # Smaller batch size due to memory constraints
per_device_eval_batch_size: 2
gradient_accumulation_steps: 2  # Increase to maintain effective batch size
learning_rate: 5e-5  # Lower learning rate for stability
weight_decay: 0.01  # Higher weight decay for regularization
warmup_ratio: 0.1  # More warmup for stability
lr_scheduler_type: "linear"
logging_steps: 10
save_steps: 250  # More frequent saves
eval_steps: 250
save_total_limit: 2  # Keep fewer checkpoints due to size
load_best_model_at_end: true
metric_for_best_model: "eval_loss"
greater_is_better: false
max_grad_norm: 0.5  # Lower gradient clipping for stability

# Memory optimization (critical for full fine-tuning)
gradient_checkpointing: true  # Essential for memory efficiency

# Monitoring
use_wandb: false
wandb_project: "full-sft-training"

# Hub upload (full model)
push_to_hub: false
hub_repo_id: "your-username/your-full-model"
hub_commit_message: "Upload full fine-tuned model"
hub_private: false
push_adapter_only: false  # Upload full model for full fine-tuning

# Additional options
resume_from_checkpoint: null
convert_to_gguf: false
gguf_quantization: "q4_0"

# ICA masking (optional)
mask_mode: null  # "key" or "complement" or null
ica_mask_path: null
ica_components: 20
ica_percentile: 98.0
ica_dtype: null  # null/float32 (stable), "auto" (match model), "float16"/"bfloat16" (faster)

# Memory and Performance Notes:
# - Full fine-tuning requires 2-4x more GPU memory than PEFT
# - Training time is typically 2-3x longer than PEFT
# - Final model size is the full model size (not just adapters)
# - Higher risk of catastrophic forgetting
# - May achieve better performance on domain-specific tasks
# - Recommended for scenarios where maximum performance is critical
#   and computational resources are abundant
