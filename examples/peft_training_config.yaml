# PEFT (Parameter-Efficient Fine-Tuning) Configuration Example
# This configuration uses LoRA/QLoRA for memory-efficient fine-tuning

# Model configuration
model_name_or_path: "microsoft/DialoGPT-medium"
use_auth_token: true
trust_remote_code: true
torch_dtype: "auto"

# Dataset configuration
dataset_name_or_path: "tatsu-lab/alpaca"
dataset_config_name: null
max_seq_length: 2048
instruction_template: "### Instruction:\n{instruction}\n\n### Response:\n{response}"
validation_split: 0.1
auto_detect_format: true
template_format: "auto"

# Quantization settings (for memory efficiency)
use_4bit: true
use_8bit: false
bnb_4bit_compute_dtype: "float16"
bnb_4bit_quant_type: "nf4"
bnb_4bit_use_double_quant: true

# PEFT/LoRA configuration
# no_peft: false  # PEFT is enabled by default, uncomment and set to true to disable
lora_r: 16
lora_alpha: 32
lora_dropout: 0.1
lora_target_modules: null  # Auto-detect
lora_bias: "none"

# Training configuration (optimized for PEFT)
output_dir: "./models/peft-finetuned"
num_train_epochs: 3
per_device_train_batch_size: 4
per_device_eval_batch_size: 4
gradient_accumulation_steps: 1
learning_rate: 2e-4  # Higher learning rate suitable for PEFT
weight_decay: 0.001
warmup_ratio: 0.03
lr_scheduler_type: "cosine"
logging_steps: 10
save_steps: 500
eval_steps: 500
save_total_limit: 3
load_best_model_at_end: true
metric_for_best_model: "eval_loss"
greater_is_better: false
max_grad_norm: 1.0

# Memory optimization
gradient_checkpointing: true

# Monitoring
use_wandb: false
wandb_project: "peft-sft-training"

# Hub upload (adapters only)
push_to_hub: false
hub_repo_id: "your-username/your-peft-model"
hub_commit_message: "Upload PEFT adapters"
hub_private: false
push_adapter_only: true  # Only upload adapters for PEFT

# Additional options
resume_from_checkpoint: null
convert_to_gguf: false
gguf_quantization: "q4_0"

# ICA masking (optional)
mask_mode: null  # "key" or "complement" or null
ica_mask_path: null
ica_components: 20
ica_percentile: 98.0
