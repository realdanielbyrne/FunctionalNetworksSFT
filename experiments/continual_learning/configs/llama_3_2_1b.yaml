# Continual Learning Evaluation Configuration
# Model: LLaMA-3.2-1B-Instruct (development model for faster iteration)

model:
  name: "llama-3.2-1b"
  model_name: "meta-llama/Llama-3.2-1B-Instruct"
  torch_dtype: "float16"
  device_map: "auto"

lora:
  r: 16
  alpha: 32
  dropout: 0.05
  target_modules:
    - "q_proj"
    - "v_proj"
    - "k_proj"
    - "o_proj"

training:
  learning_rate: 1e-4
  batch_size: 8
  num_steps_per_task: 1000
  gradient_clip: 1.0
  warmup_ratio: 0.1

evaluation:
  max_seq_length: 512
  num_seeds: 3
  lm_eval_benchmarks:
    - "mmlu"

# Task orders to run (DOC Table 7)
task_orders:
  standard:
    - "order_1"
    - "order_2"
    - "order_3"
  long_chain:
    - "order_4"
    - "order_5"
    - "order_6"

# CL methods with per-method hyperparameters
methods:
  lora: {}
  ewc:
    ewc_lambda: 0.4
  lwf:
    lwf_alpha: 1.0
    temperature: 2.0
  o_lora:
    ortho_lambda: 0.1
  doc:
    doc_lambda: 0.5
    subspace_fraction: 0.1

# ICA variations
ica_variations:
  ica_lesion:
    mask_mode: "lesion"
    anti_drift: false
  ica_preserve:
    mask_mode: "preserve"
    anti_drift: false
  ica_lesion_antidrift:
    mask_mode: "lesion"
    anti_drift: true
  ica_preserve_antidrift:
    mask_mode: "preserve"
    anti_drift: true

# ICA global configuration
ica:
  components: 10
  percentile: 98.0
  template_path: null

output:
  results_dir: "./experiments/continual_learning/results"
  save_models: false
