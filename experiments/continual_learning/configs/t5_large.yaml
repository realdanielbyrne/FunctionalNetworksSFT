# Continual Learning Evaluation Configuration
# Model: T5-Large (skipped for this study -- focus on decoder-only architectures)

model:
  name: "t5-large"
  model_name: "google/t5-large"
  torch_dtype: "float32"  # T5 works better with float32
  device_map: "auto"

lora:
  r: 16
  alpha: 32
  dropout: 0.05
  target_modules:
    - "q"
    - "v"

training:
  learning_rate: 1e-3  # Higher LR for T5
  batch_size: 64       # Larger batch size for T5
  num_steps_per_task: 1000
  gradient_clip: 1.0
  warmup_ratio: 0.1

evaluation:
  max_seq_length: 512
  num_seeds: 3
  lm_eval_benchmarks:
    - "mmlu"

# Task orders to run (DOC Table 7)
task_orders:
  standard:
    - "order_1"
    - "order_2"
    - "order_3"
  long_chain:
    - "order_4"
    - "order_5"
    - "order_6"

# CL methods
methods:
  lora: {}
  ewc:
    ewc_lambda: 0.4
  lwf:
    lwf_alpha: 1.0
    temperature: 2.0
  o_lora:
    ortho_lambda: 0.1
  doc:
    doc_lambda: 0.5
    subspace_fraction: 0.1

# ICA global configuration
ica:
  components: 10
  percentile: 98.0
  template_path: null

output:
  results_dir: "./experiments/continual_learning/results"
  save_models: false
