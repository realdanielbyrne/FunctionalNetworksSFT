# Continual Learning Evaluation Configuration
# Model: T5-Large (matches DOC paper setup)

model:
  name: "t5-large"
  model_name: "google/t5-large"
  torch_dtype: "float32"  # T5 works better with float32
  device_map: "auto"

lora:
  r: 16
  alpha: 32
  dropout: 0.05
  target_modules:
    - "q"
    - "v"

training:
  learning_rate: 1e-3  # Higher LR for T5
  batch_size: 64       # Larger batch size for T5
  num_steps_per_task: 1000
  gradient_clip: 1.0
  warmup_ratio: 0.1

evaluation:
  max_seq_length: 512
  seed: 42

# Task orders to run
task_orders:
  standard:
    - "order_1"
    - "order_2"
    - "order_3"
  long_chain:
    - "order_4"
    - "order_5"
    - "order_6"

# Methods to compare
methods:
  - "lora"         # Baseline (catastrophic forgetting)
  - "ewc"          # Elastic Weight Consolidation
  - "lwf"          # Learning without Forgetting
  - "o_lora"       # Orthogonal LoRA
  - "doc"          # Dynamic Orthogonal Continual
  - "ica_networks" # ICA-based functional networks

# ICA-specific configuration
ica:
  components: 10
  percentile: 98.0
  mask_mode: "lesion"  # or "preserve"
  template_path: null  # Path to precomputed templates

output:
  results_dir: "./experiments/continual_learning/results"
  save_models: false
