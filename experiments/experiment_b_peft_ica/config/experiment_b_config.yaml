# =============================================================================
# Experiment B: PEFT + ICA Masking Fine-Tuning Configuration
# =============================================================================
# This configuration is for fine-tuning meta-llama/Llama-3.2-1B-Instruct
# using PEFT (LoRA) with ICA masking enabled.
# 
# Experiment Parameters:
# - Model: meta-llama/Llama-3.2-1B-Instruct
# - Dataset: datasets/sarcasm.csv
# - Training Method: PEFT (LoRA) + ICA masking
# - Epochs: 2
# - ICA Masking: Enabled (key mode)
# =============================================================================

# Model configuration - MODIFIED FOR TEST RUN
model_name_or_path: "microsoft/DialoGPT-small"  # TEST RUN: Much smaller model
torch_dtype: "float32"  # Use float32 for MPS compatibility

# Dataset configuration
dataset_name_or_path: "datasets/sarcasm.csv"
validation_split: 0.1  # 10% for validation

# Output configuration
output_dir: "experiments/experiment_b_peft_ica/output"

# PEFT Configuration (LoRA) - IDENTICAL TO EXPERIMENT A
# PEFT is enabled by default - no need to set peft: true
lora_r: 16              # LoRA rank
lora_alpha: 32          # LoRA alpha scaling
lora_dropout: 0.1       # LoRA dropout
lora_target_modules: null  # Auto-detect target modules

# Training Configuration - MODIFIED FOR TEST RUN
max_steps: 5            # TEST RUN: Only 5 steps for validation
num_train_epochs: 2     # Will be overridden by max_steps
per_device_train_batch_size: 1  # TEST RUN: Reduced for memory
per_device_eval_batch_size: 1   # TEST RUN: Reduced for memory
gradient_accumulation_steps: 1
learning_rate: 2e-4     # Higher learning rate for PEFT
weight_decay: 0.001
warmup_ratio: 0.03
lr_scheduler_type: "cosine"

# Sequence and Template Configuration - MODIFIED FOR TEST RUN
max_seq_length: 256     # TEST RUN: Reduced for memory
template_format: "auto" # Auto-detect chat template

# Evaluation and Logging - MODIFIED FOR TEST RUN
logging_steps: 1        # TEST RUN: Log every step
save_steps: 10          # TEST RUN: Save more frequently
eval_steps: 3           # TEST RUN: Evaluate after 3 steps
save_total_limit: 2
load_best_model_at_end: true
metric_for_best_model: "eval_loss"
evaluation_strategy: "steps"

# Memory optimization - MODIFIED FOR TEST RUN
gradient_checkpointing: false  # TEST RUN: Disabled for MPS compatibility

# Monitoring
use_wandb: false
wandb_project: "experiment-b-peft-ica"

# Hub upload (adapters only)
push_to_hub: false
hub_repo_id: "experiment-b-peft-ica-sarcasm"
hub_commit_message: "Experiment B: PEFT+ICA sarcasm fine-tuning"
hub_private: false
push_adapter_only: true  # Only upload adapters for PEFT

# Additional options - IDENTICAL TO EXPERIMENT A
resume_from_checkpoint: null
convert_to_gguf: false
gguf_quantization: "q4_0"

# ICA masking (ENABLED for Experiment B) - THE ONLY DIFFERENCE
mask_mode: "key"        # Enable ICA masking in "key" mode
ica_mask_path: null     # Compute masks on-the-fly
ica_components: 20      # Number of ICA components
ica_percentile: 98.0    # Percentile threshold for neuron selection

# Authentication
use_auth_token: true    # For gated model access
