# =============================================================================
# Experiment B: PEFT + ICA Masking Fine-Tuning Configuration
# =============================================================================
# This configuration is for fine-tuning meta-llama/Llama-3.2-1B-Instruct
# using PEFT (LoRA) with ICA masking enabled.
# 
# Experiment Parameters:
# - Model: meta-llama/Llama-3.2-1B-Instruct
# - Dataset: databricks/databricks-dolly-15k
# - Training Method: PEFT (LoRA) + ICA masking
# - Epochs: 2
# - ICA Masking: Enabled (lesion mode)
# =============================================================================

# Model configuration
#model_name_or_path: "microsoft/DialoGPT-small"

#model_name_or_path: "Qwen/Qwen3-0.6B"
#model_name_or_path: "unsloth/Llama-3.2-3B-bnb-4bit"
#model_name_or_path: "meta-llama/Llama-3.2-3B-Instruct"
model_name_or_path: "experiments/peft_vs_peft-ica/experiment_b_peft_ica/output/components_0_1/first_ft/merged_model"

# Output configuration
output_dir: "experiments/peft_vs_peft-ica/experiment_b_peft_ica/output/components_0_1/second_ft"
torch_dtype: "auto"  # Use platform-optimized dtype

# Dataset configuration
#dataset_name_or_path: "databricks/databricks-dolly-15k"
dataset_name_or_path: "camel-ai/physics"
#dataset_name_or_path: "YaTharThShaRma999/Physics_dataset"
#dataset_name_or_path: "0xZee/dataset-CoT-Atomic-Nuclear-Physics-144"
#dataset_name_or_path: "datasets/sarcasm.csv"
validation_split: 0.1  # 10% for validation

# Dataset preprocessing parameters
response_max_length: 3500     # Maximum response field length (aligned with Exp A)
instruction_max_length: 3200  # Maximum combined instruction length (including context, aligned with Exp A)


# PEFT Configuration (LoRA) - IDENTICAL TO EXPERIMENT A
# PEFT is enabled by default - no need to set peft: true
lora_r: 16              # LoRA rank
lora_alpha: 32          # LoRA alpha scaling
lora_dropout: 0       # LoRA dropout (aligned with Exp A)
lora_target_modules: null  # Auto-detect target modules

# Training Configuration
#max_steps: 5 # Uncomment to override num_train_epochs
num_train_epochs: 4     # Exactly 2 epochs as specified
per_device_train_batch_size: 1
per_device_eval_batch_size: 1
gradient_accumulation_steps: 1
learning_rate: 0.0005   # Higher learning rate for PEFT
weight_decay: 0.0001
warmup_ratio: 0.05
lr_scheduler_type: "cosine"

# Sequence and Template Configuration
max_seq_length: 4196
template_format: "auto" # Auto-detect chat template

# Evaluation and Logging
logging_steps: 100
save_steps: 500
eval_steps: 500
save_total_limit: 2
load_best_model_at_end: true
metric_for_best_model: "eval_loss"
evaluation_strategy: "steps"

# Memory optimization
#gradient_checkpointing: true
gradient_checkpointing: false

# Monitoring
use_wandb: true
wandb_project: "FnSFT_PEFT+ICA_Lesion"

# Hub upload (merged model)
push_to_hub: false
hub_repo_id: "realdanielbyrne/Llama-3.2-3B-Instruct_PEFT+ICA_Lesion"
hub_commit_message: "Llama-3.2-3B-Instruct_PEFT+ICA_Lesion"
hub_private: false
merge_adapter_with_base: true  # Merge adapter with base model
upload_merged_model: true      # Upload merged model to Hub instead of adapter
resume_from_checkpoint: null
convert_to_gguf: false


# ICA masking (ENABLED for Experiment B) - THE ONLY DIFFERENCE
mask_mode: "lesion"        # Enable ICA masking in 'lesion' mode
ica_template_path: "ica_templates/llama-3.2-3b/camel-ai_physics/global_templates.json"
ica_components: 5      # Number of ICA components
ica_percentile: 98.0    # Percentile threshold for neuron selection
ica_component_ids: [0, 1]

# Authentication
use_auth_token: true    # For gated model access

attn_implementation: "sdpa"