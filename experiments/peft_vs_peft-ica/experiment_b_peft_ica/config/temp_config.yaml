attn_implementation: sdpa
convert_to_gguf: false
dataset_name_or_path: camel-ai/physics
eval_steps: 5000
evaluation_strategy: steps
gradient_accumulation_steps: 1
gradient_checkpointing: false
hub_commit_message: Experiment B - Components [0,1,2] masked
hub_private: false
hub_repo_id: realdanielbyrne/Llama-3.2-3B-Instruct_PEFT+ICA_Lesion
ica_component_ids:
- 0
- 1
- 2
ica_components: 5
ica_percentile: 98.0
ica_template_path: ica_templates/llama-3.2-3b/camel-ai_physics/global_templates.json
instruction_max_length: 3200
learning_rate: 0.0005
load_best_model_at_end: true
logging_steps: 100
lora_alpha: 32
lora_dropout: 0
lora_r: 16
lora_target_modules: null
lr_scheduler_type: cosine
mask_mode: lesion
max_seq_length: 4196
merge_adapter_with_base: true
metric_for_best_model: eval_loss
model_name_or_path: meta-llama/Llama-3.2-3B-Instruct
num_train_epochs: 2
output_dir: experiments/peft_vs_peft-ica/experiment_b_peft_ica/output/components_0_1_2/first_ft
per_device_eval_batch_size: 1
per_device_train_batch_size: 1
push_to_hub: false
response_max_length: 3500
resume_from_checkpoint: null
save_steps: 5000
save_total_limit: 2
template_format: auto
torch_dtype: auto
upload_merged_model: true
use_auth_token: true
use_wandb: true
validation_split: 0.1
wandb_project: FnSFT_PEFT+ICA_Lesion
wandb_run_name: exp_b_components_0_1_2
warmup_ratio: 0.05
weight_decay: 0.0001
