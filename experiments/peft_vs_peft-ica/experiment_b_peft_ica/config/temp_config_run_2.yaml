attn_implementation: sdpa
convert_to_gguf: false
dataset_name_or_path: databricks/databricks-dolly-15k
eval_steps: 3000
evaluation_strategy: steps
gradient_accumulation_steps: 2
gradient_checkpointing: false
hub_commit_message: Experiment B - Components [0,1] masked
hub_private: false
hub_repo_id: Llama-3.2-1B-Instruct
ica_component_ids:
- 0
- 1
ica_components: 10
ica_percentile: 98.0
ica_template_path: ica_templates/llama-3.2-1b/databricks_databricks-dolly-15k/global_templates.json
instruction_max_length: 2800
learning_rate: 0.0005
load_best_model_at_end: true
logging_steps: 100
lora_alpha: 32
lora_dropout: 0.1
lora_r: 16
lora_target_modules: null
lr_scheduler_type: cosine
mask_mode: lesion
max_seq_length: 4096
merge_adapter_with_base: true
metric_for_best_model: eval_loss
model_name_or_path: meta-llama/Llama-3.2-1B-Instruct
num_train_epochs: 2
output_dir: experiments/peft_vs_peft-ica/experiment_b_peft_ica/output/run_2_components_0_1
per_device_eval_batch_size: 1
per_device_train_batch_size: 2
push_to_hub: true
response_max_length: 3500
resume_from_checkpoint: null
save_steps: 3000
save_total_limit: 1
template_format: auto
torch_dtype: auto
upload_merged_model: true
use_auth_token: true
use_wandb: true
validation_split: 0.1
wandb_project: Fnsft_PEFT_vs_PEFT+ICA
wandb_run_name: exp_b_components_0_1
warmup_ratio: 0.03
weight_decay: 0.0001
